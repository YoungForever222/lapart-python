
@inproceedings{jones_photovoltaic_2015,
	address = {New Orleans, LA, USA},
	title = {Photovoltaic system fault detection and diagnostics using laterally primed adaptive resonance theory neural network},
	author = {Jones, C. Birk and Stein, Joshua S. and Gonzalez, Sigifredo and King, Bruce H.},
	month = jun,
	year = {2015}
}

@article{carpenter_art_1987,
	title = {{ART} 2: self-organization of stable category recognition codes for analog input patterns},
	volume = {26},
	copyright = {© 1987 Optical Society of America},
	issn = {2155-3165},
	shorttitle = {{ART} 2},
	url = {https://www.osapublishing.org/abstract.cfm?uri=ao-26-23-4919},
	doi = {10.1364/AO.26.004919},
	abstract = {Adaptive resonance architectures are neural networks that self-organize stable pattern recognition codes in real-time in response to arbitrary sequences of input patterns. This article introduces ART 2, a class of adaptive resonance architectures which rapidly self-organize pattern recognition categories in response to arbitrary sequences of either analog or binary input patterns. In order to cope with arbitrary sequences of analog input patterns, ART 2 architectures embody solutions to a number of design principles, such as the stability-plasticity tradeoff, the search-direct access tradeoff, and the match-reset tradeoff. In these architectures, top-down learned expectation and matching mechanisms are critical in self-stabilizing the code learning process. A parallel search scheme updates itself adaptively as the learning process unfolds, and realizes a form of real-time hypothesis discovery, testing, learning, and recognition. After learning self-stabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time for familiar inputs does not increase with the complexity of the learned code. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. A parameter called the attentional vigilance parameter determines how fine the categories will be. If vigilance increases (decreases) due to environmental feedback, then the system automatically searches for and learns finer (coarser) recognition categories. Gain control parameters enable the architecture to suppress noise up to a prescribed level. The architecture's global design enables it to learn effectively despite the high degree of nonlinearity of such mechanisms.},
	language = {EN},
	number = {23},
	urldate = {2017-07-09},
	journal = {Applied Optics},
	author = {Carpenter, Gail A. and Grossberg, Stephen},
	month = dec,
	year = {1987},
	pages = {4919--4930},
	file = {Full Text PDF:/Users/cbjones/Library/Application Support/Zotero/Profiles/intsee01.default/zotero/storage/VRXQEIHT/Carpenter and Grossberg - 1987 - ART 2 self-organization of stable category recogn.pdf:application/pdf;Snapshot:/Users/cbjones/Library/Application Support/Zotero/Profiles/intsee01.default/zotero/storage/HVECQCR3/abstract.html:text/html}
}

@article{carpenter_massively_1987,
	title = {A massively parallel architecture for a self-organizing neural pattern recognition machine},
	volume = {37},
	issn = {0734-189X},
	url = {http://www.sciencedirect.com/science/article/pii/S0734189X87800142},
	doi = {10.1016/S0734-189X(87)80014-2},
	abstract = {A neural network architecture for the learning of recognition categories is derived. Real-time network dynamics are completely characterized through mathematical analysis and computer simulations. The architecture self-organizes and self-stabilizes its recognition codes in response to arbitrary orderings of arbitrarily many and arbitrarily complex binary input patterns. Top-down attentional and matching mechanisms are critical in self-stabilizing the code learning process. The architecture embodies a parallel search scheme which updates itself adaptively as the learning process unfolds. After learning self-stabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time does not grow as a function of code complexity. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. These invariant properties emerge in the form of learned critical feature patterns, or prototypes. The architecture possesses a context-sensitive self-scaling property which enables its emergent critical feature patterns to form. They detect and remember statistically predictive configurations of featural elements which are derived from the set of all input patterns that are ever experienced. Four types of attentional process—priming, gain control, vigilance, and intermodal competition—are mechanistically characterized. Top—down priming and gain control are needed for code matching and self-stabilization. Attentional vigilance determines how fine the learned categories will be. If vigilance increases due to an environmental disconfirmation, then the system automatically searches for and learns finer recognition categories. A new nonlinear matching law (the ⅔ Rule) and new nonlinear associative laws (the Weber Law Rule, the Associative Decay Rule, and the Template Learning Rule) are needed to achieve these properties. All the rules describe emergent properties of parallel network interactions. The architecture circumvents the noise, saturation, capacity, orthogonality, and linear predictability constraints that limit the codes which can be stably learned by alternative recognition models.},
	number = {1},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Carpenter, Gail A. and Grossberg, Stephen},
	month = jan,
	year = {1987},
	pages = {54--115},
	file = {ScienceDirect Snapshot:/Users/cbjones/Library/Application Support/Zotero/Profiles/intsee01.default/zotero/storage/PIPERCTA/S0734189X87800142.html:text/html}
}